\documentclass[11pt]{article}

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{graphicx}

\newtheorem{theorem}{Theorem}
\newtheorem*{pathlemma}{Path Lemma}
\newtheorem*{etreelemma}{Etree Fill Lemma}
\newcommand{\m}[1]{{\bf{#1}}}       % for matrices and vectors
\newcommand{\ones}{\m1}             % vector of all ones
\newcommand{\zeros}{\m0}            % vector of all zeros
\newcommand{\diag}{\mbox{diag}}
\newcommand{\Diag}{\mbox{Diag}}
\newcommand{\dilation}{\mbox{dilation}}
\newcommand{\congestion}{\mbox{congestion}}
\newcommand{\stretchh}{\mbox{stretch}}
\newcommand{\Reff}{R^{\mbox{\scriptsize eff}}}  % effective resistance
\newcommand{\pinv}{^{\dagger}}                  % pseudoinverse
\newcommand{\Real}{\mathbb{R}}      % real numbers
\newcommand{\krylov}{\mathcal{K}}   % Krylov subspace

\topmargin 0in
\textheight 7.9in
\oddsidemargin 0pt
\evensidemargin 0pt
\textwidth 6.5in

\begin{document}

\title{Index of Notation and Definitions}
\author{CS 292F: Graph Laplacians and Spectra}
\date{Version of April 7, 2021}
\maketitle

There is a lot of variation in terminology and notation in
the field of Laplacian matrix computation and spectral graph 
theory.  
Indeed, even ``Laplacian matrix'' is defined differently by
different authors!

This list gives the versions of notation, terminology, and definitions 
that we will use in CS 292F.
I mostly follow the conventions of Dan Spielman's notes, 
though I prefer not to use greek letters for vectors.
I will keep adding to this list during the quarter.

\begin{enumerate}

\item
Unless otherwise stated, a {\em graph} $G = (V,E)$ is always 
an undirected graph whose $n$ vertices are the integers 
$1$ through $n$, with no multiple edges or loops.

\item
The {\em degree} of a vertex is the number of edges incident on it, 
or equivalently (because we don't allow multiple edges or loops)
the number of its neighboring vertices.

\item 
A graph is said to be {\em regular} if every vertex has the same degree.

\item
A graph is said to be {\em connected} if, for every choice of two
vertices $i$ and $j$, there is a {\em path} of edges from $i$ to $j$.
The {\em connected components} of a graph are its maximal connected
subgraphs.

\item
$K_n$ is the {\em complete graph}, which has $n$ vertices and all $n(n-1)/2$ possible edges.

\item
$P_n$ is the {\em path graph}, which has $n$ vertices and $n-1$ edges in a single path.

\item
$S_n$ is the {\em star graph}, which has $n$ vertices, one with degree $n-1$ and 
$n-1$ with degree 1.

\item
$H_k$ is the {\em hypercube graph}, which has $n=2^k$ vertices, all of degree $k$.
Vertices $i$ and $j$ have an edge between them if $i$ and $j$ differ by a power of 2.
Equivalently, we can identify each vertex with a subset of $\{1,\ldots,k\}$,
with edges to just those subsets formed by adding or deleting one element.

\item
$G_e$ or $G_{(i,j)}$ is the graph with $n$ vertices and only one edge $e = (i,j)$.

\item
We will write a {\em vector} as a lower-case latin letter, 
possibly with a subscript, like $x$ or $w_2$.  
We often think of an $n$-vector as a set of labels for the
$n$ vertices of a graph; 
in that case element $i$ of vector $x$ is written as $x(i)$,
and we may write $x\in\Real^V$ instead of $x\in\Real^n$.
In linear algebraic expressions, vectors are column vectors.

\item
Two special vectors are $\zeros$, the vector of all zeros,
and $\ones$, the vector of all ones.

\item
If $i$ is a vertex then $\ones_i$~is the 
{\em characteristic vector} of~$i$, which
is zero except for $\ones_i(i)=1$.
Similarly if $S$~is a set of vertices, 
then $\ones_S$ is the vector that is equal to one
on the elements of~$S$ and zero elsewhere.

\item
If $x$ and $y$ are vectors of the same dimension, 
$$x^Ty = y^Tx = \sum_{i=0}^{n}x(i)y(i)$$
is their inner product (or dot product).
Thus $\ones^T\!x$ is the sum of the elements of $x$,
and $x^Tx$ is the square of the 2-norm (Euclidean length) of $x$.
If $x^Ty=0$, we call $x$ and $y$ {\em orthogonal}, 
and they are in fact perpendicular as vectors in $\Real^n$.

\item
If $d$ is an $n$-vector, $\Diag(d)$ is the $n$-by-$n$ diagonal 
matrix with the elements of $d$ on the diagonal.
If $A$ is any $n$-by-$n$ matrix, $\diag(A)$ is the $n$-vector
of the diagonal elements of $A$.

\item\label{lap}
The {\em Laplacian} of graph $G$ is the $n$-by-$n$ matrix $L$
whose diagonal element $L(i,i)$ is the degree of vertex $i$, 
and whose off-diagonal element $L(i,j)$ is~$-1$ if $(i,j) \in E$ 
and $0$ if $(i,j) \notin E$.
This matrix, which we (and Spielman) just call the Laplacian,
is sometimes called the {\em combinatorial Laplacian} to 
distinguish it from the normalized Laplacian below~(\ref{nlap}).
Note that $ L\ones = \zeros$.

\item
$L_e$ or $L_{(i,j)}$ is the $n$-by-$n$ Laplacian matrix
of the graph with $n$ vertices and only one edge $e = (i,j)$.
This matrix has only four nonzero elements, two 1's on the
diagonal and two $-1$'s in positions $(i,j)$ and $(j,i)$;
thus 
$$L_{(i,j)}=(\ones_i-\ones_j)(\ones_i-\ones_j)^T.$$
The Laplacian of any graph $G=(V,E)$ is the sum of the Laplacians
of its edges,
$$L_G = \sum_{e\in E} L_e.$$

\item\label{LQF}
The {\em Laplacian quadratic form} (or just LQF) is $x^TLx$,
where $L$ is a particular graph's Laplacian and $x$ is a variable $n$-vector.
Its value for a particular vector $x$ is 
$$x^TLx = \sum_{(i,j)\in E}(x(i)-x(j))^2.$$

\item
A {\em cut vector} is a vector each of whose elements is $+1$ or $-1$.
We can think of a cut vector~$x$ as representing a {\em cut} that partitions
the vertices of graph into two sets $S = \{\,i : x(i) = 1\,\}$
and $V-S = \{\,i : x(i) = -1\,\}$.
The LQF evaluated at a cut vector is easily seen to be four times the number 
of edges that cross the cut:
$$ x^TLx = 4\cdot|\{\,(i,j)\in E : i\in S \wedge j\in V-S\,\}|.$$

\item\label{eig}
If $Aw=\lambda w$ for any square matrix~$A$, nonzero vector~$w$, and scalar~$\lambda$, 
then $\lambda$ is an {\em eigenvalue} of~$A$ and $w$~is an {\em eigenvector}
associated with~$\lambda$.

\item
If $A$ is square and $B$~is nonsingular, then the eigenvalues of $BAB^{-1}$ are the
same as those of~$A$, 
and the eigenvectors of $BAB^{-1}$ are $B$ times the eigenvectors of~$A$.

\item
Every Laplacian $L$ is {\em positive semidefinite}, 
which (along with symmetry) implies that its $n$ eigenvalues are nonnegative and real.
Zero is an eigenvalue of $L$ with multiplicity
equal to the number of connected components of the graph $G$.  
Therefore,
if $G$ is connected, we have $0 = \lambda_1 < \lambda_2 \leq \cdots \leq \lambda_n$.
In that case the eigenvector $w_1$ is the constant vector $\ones/\sqrt n$.

\item
The {\em Fiedler value} of a graph is $\lambda_2$, its second-smallest eigenvalue,
and the {\em Fiedler vector} is $w_2$, the associated eigenvector.
The Fiedler value of a graph is also called its {\em algebraic connectivity}.
Note that $\lambda_2=0$ iff the graph is not connected.

\item
A square matrix $Q$ is {\em orthogonal} if $Q^TQ=I$, that is, its inverse
is its transpose.  
As vectors, the columns of $Q$ have unit length and are pairwise perpendicular; 
the same is true of the rows of $Q$.

\item\label{symeig}
If the $n$-by-$n$ matrix $A$ is symmetric, then it possesses $n$ real eigenvalues
$\lambda_1\leq\lambda_2\leq\cdots\leq\lambda_n$ (possibly including duplicates) 
associated with $n$ mutually orthogonal unit-length eigenvectors
$w_1, w_2, \ldots, w_n$.  If $W$ is the matrix $[w_1\; w_2\; \ldots\; w_n]$
and $\Lambda$ is the matrix $\Diag(\lambda_1,\ldots,\lambda_n)$
then we can summarize this as $AW = W\Lambda$ and $W^TW=I$.
We also have $A=W\Lambda W^T$, whence
$$A = \sum_{i=1}^n \lambda_i w_iw_i^T.$$

\item
If symmetric $A$ and its eigenvalues and eigenvectors are as in~(\ref{symeig}), 
any vector $x$ can be written as a linear combination of eigenvectors,
$$x = \sum_{i=1}^n \alpha_i w_i,$$
where $\alpha_i = w_i^Tx.$  
Multiplication by $A$ acts termwise on such a sum:
$$A^k x = \sum_{i=1}^n \alpha_i\lambda_i^k w_i.$$

\item
If symmetric $A$ and its eigenvalues and eigenvectors are as in~(\ref{symeig}), 
the {\em pseudoinverse} of $A$ is
$$A\pinv = \sum_{\lambda_i\ne 0} \frac{1}{\lambda_i} w_iw_i^T,$$
where the sum is taken over the nonzero eigenvalues of $A$.
If $A$ is nonsingular, $A\pinv=A^{-1}$.
If $x$ is orthogonal to the null space of $A$ 
(i.e.\ $x^Tw_i=0$ whenever $\lambda_i=0$), then
$$A\pinv A x = AA\pinv x = x.$$

\item
The {\em positive semidefinite square root} of a positive semidefinite matrix~$A$
with eigenvalues and eigenvectors as in~(\ref{symeig}) is the matrix
$$A^{1/2} = \sum_{i=1}^n \lambda_i^{1/2} w_iw_i^T.$$
We write the psd square root of~$A\pinv$ as 
$$A^{\dagger/2} = \sum_{\lambda_i\ne 0} \lambda_i^{-1/2} w_iw_i^T.$$

\item
The {\em Rayleigh quotient} of a nonzero vector $x$ and a matrix $A$ is
$$\frac{x^TAx}{x^Tx}.$$
If $Ax=\lambda x$, then the Rayleigh quotient of $x$ and $A$ is $\lambda$.

\item\label{RQT}{\bf Rayleigh quotient theorem}.
The eigenvectors of a symmetric matrix $A$ are critical points of its
Rayleigh quotient (considered as a real-valued function of an $n$-vector).
Specifically, 
$$\lambda_k = \min_{x\perp w_1,\ldots,w_{k-1}} \frac{x^TAx}{x^Tx}
            = \max_{x\perp w_{k+1},\ldots,w_n} \frac{x^TAx}{x^Tx},$$
and the extreme values are attained at $x = w_k$.
In particular, therefore, for a Laplacian $L$ the Fiedler value is
$$\lambda_2 = \min_{\ones^T\!x=0} \frac{x^TAx}{x^Tx},$$
attained at the Fiedler vector $w_2$.

\item\label{CFT}{\bf Courant-Fischer theorem}
(a stronger version of the Rayleigh quotient theorem).
The eigenvalues $\lambda_1\leq\cdots\leq\lambda_n$ of a symmetric matrix $A$ 
are characterized by
$$\lambda_k = \max_{\dim \mathbb{S}=n-k+1}\;\min_{x\in \mathbb{S}}\frac{x^TAx}{x^Tx}
            = \min_{\dim \mathbb{S}=k}\;\max_{x\in \mathbb{S}}\frac{x^TAx}{x^Tx},$$
where $\mathbb{S}$ ranges over subspaces of $\Real^n$.
The extreme values are attained at $x = w_k$.

\item
A {\em test vector for $\lambda_2$} is an $n$-vector that is
orthogonal to $\ones$.  By the Raleigh quotient theorem,
if $v$ is any test vector then $\lambda_2 \leq v^TLv/v^Tv$.
Note that any vector $x$ can be converted to a test vector 
$v = x - (\ones^T\!x/n) \ones$; in words, subtracting off
the mean of any vector orthogonalizes it against the constant
vector.

\item
The {\em boundary} of a set $S\subseteq V$ of vertices, written $\partial S$,
is the set of edges with just one endpoint in $S$. Formally,
$\partial S = \{\,(i,j)\in E : i \in S \wedge j \in V-S\,\}$.
The number of edges in $\partial S$ is $|\partial S|$.

\item\label{isop}
The {\em isoperimetric ratio} of a set $S\subseteq V$ of vertices,
written $\theta(S)$, is the ratio
$$\theta(S) = \frac{|\partial S|}{|S|}.$$
This is one sort of ``surface-to-volume ratio''; 
see the definition of conductance~(\ref{cond}) 
for another.

\item\label{isopnum}
The {\em isoperimetric ratio} of a graph $G$,
written $\theta_G$, is the smallest isoperimetric ratio over all 
sets with at most half the vertices,
$$\theta_G = \min_{|S|\leq n/2}\theta(S).$$
Note that $\theta_G=0$ if and only if $G$ is not connected.

\item\label{isopthm} {\bf Isoperimetric theorem}.
For any set $S$ of vertices, 
$$\theta(S) \geq \lambda_2(1-|S|/n).$$
It follows that the isoperimetric ratio of the graph is bounded 
in terms of the Fiedler value,
$$\theta_G \geq \lambda_2/2.$$
This says that the larger $\lambda_2$ is, the larger the surface-to-volume
ratio of any relatively small set of vertices must be.

\item\label{cond}
The {\em conductance} of a set $S\subseteq V$ of vertices,
written $\phi(S)$, is the ratio 
$$\phi(S) = \frac{|\partial S|}{\min(d(S),d(V-S))},$$
where $d(S)$ is the sum of the degrees of the vertices in $S$.
This is another sort of ``surface-to-volume ratio'';
isoperimetric number~(\ref{isopnum}) measures volume just by counting vertices, 
while conductance measures volume by counting vertices weighted by their degrees.
(``Conductance'' has a different meaning in resistive networks, as we'll see later.)
%; see~(\ref{resistive}) below.)

\item
The {\em conductance} of a graph $G$,
written $\phi_G$, is the smallest conductance of any nonempty 
proper subset of vertices,
$$\phi_G = \min_{S\subset V}\phi(S).$$
This is sometimes called the ``Cheeger constant''
of the graph, but definitions are particularly
variable here and we'll stick to this one.
Note that $\phi_G=0$ iff $G$ is not connected.
(``Conductance'' has a different meaning in resistive networks, as we'll see later.)
%; see~(\ref{resistive}) below.)

\item\label{nlap}
The {\em normalized Laplacian} of graph~$G$ is 
the $n$-by-$n$ matrix~$N$
whose diagonal element $N(i,i)$ is equal to 1,
and whose off-diagonal element $N(i,j)$ 
is~$-1/\sqrt{d(i)d(j)}$, 
where we define $d$ to be the vector of vertex degrees of~$G$.
Another way to say it is that the normalized Laplacian is 
the (ordinary) Laplacian with rows and columns scaled symmetrically 
to make the diagonal elements equal to~1. 
If $D=\diag(d)$ is the diagonal matrix of degrees, 
then 
$$N = D^{-1/2}LD^{-1/2}.$$
Some authors, including notably Fan Chung in her 
wonderful book {\em Spectral Graph Theory}, 
use the name ``Laplacian'' for this matrix~$N$
instead of for our~$L$.

\item\label{nlapeig}
The normalized Laplacian $N$ is symmetric and 
positive semidefinite, and like the Laplacian
it has 0 as an eigenvalue with multiplicity
equal to the number of connected components
of~$G$.  In general however $N$'s eigenvalues
and eigenvectors are different from $L$'s.
We write $0=\nu_1\le\nu_2\le\cdots\le\nu_n$
for the eigenvalues of $N$.
The eigenvector corresponding to $\nu_1$ is
not the constant vector, but the vector $d^{1/2}$
of the square roots of the vertex degrees:
$$Nd^{1/2} = D^{-1/2}LD^{-1/2}d^{1/2} = D^{-1/2}L\ones = D^{-1/2}\zeros = \zeros.$$

\item\label{nlaprq}
The Rayleigh quotient for $N_G$, whose critical points determine the eigenvalues,
is related to a ``generalized Rayleigh quotient'' for the Laplacian $L_G$.
Specifically, we have
$$\frac{x^TN_Gx}{x^Tx} = \frac{y^TL_Gy}{y^TDy},$$
where $D=\Diag(d)$ is the diagonal matrix of vertex degrees 
and $y=D^{-1/2}x$.
Thus the eigenvalues of $N_Gx=\nu x$ come from
the generalized eigenvalue problem $L_Gy=\nu Dy.$

\item\label{gershgorin}{\bf Gershgorin's theorem}.
If $A$ is any square matrix (real or complex), its $n$~eigenvalues are all contained
in the union of the $n$~disks $D_1,\ldots,D_n$ in the complex plane defined by
$$D_i = \{\alpha : |\alpha-A(i,i)| \leq \sum_{j\neq i}|A(i,j)|\}.$$
This implies, for example, that the largest eigenvalue $\lambda_n$ of a Laplacian
is at most twice the maximum vertex degree.

\item
It follows from Gershgorin's theorem~(\ref{gershgorin}) that
the eigenvalues of the normalized Laplacian $N$ are always bounded by~0 and~2,
$$0 = \nu_1 \leq \nu_2 \leq \cdots \leq \nu_n \leq 2.$$

\item\label{cheeger} {\bf Cheeger's inequalities}.
The normalized Laplacian can be used to give both 
upper and lower bounds on the conductance,
$$\nu_2/2 \leq \phi_G \leq \sqrt{2\nu_2}.$$
Equivalently,
$$\phi_G^2/2 \leq \nu_2 \leq 2\phi_G.$$
The upper bound on $\nu_2$ is analogous to the
isoperimetric inequality~(\ref{isopthm}).
The lower bound on $\nu_2$ is Cheeger's inequality,
one of the most significant theorems of spectral
graph theory.

\item\label{csi} {\bf Cauchy-Schwarz inequality}.
Just for reference, because it comes up in several of
the proofs we're looking at.  
If $x$ and $y$ are $n$-vectors, then 
$$|x^Ty| \leq \|x\|\,\|y\|.$$
Equivalently,
$$\Big(\sum_i x(i)y(i)\Big)^2 \leq 
\Big(\sum_i x(i)^2\Big)
\Big(\sum_i y(i)^2\Big).$$


\end{enumerate}

\end{document}
