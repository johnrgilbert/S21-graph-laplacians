\documentclass[11pt]{article}

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{graphicx}

\newtheorem{theorem}{Theorem}
\newtheorem*{pathlemma}{Path Lemma}
\newtheorem*{etreelemma}{Etree Fill Lemma}
\newcommand{\m}[1]{{\bf{#1}}}       % for matrices and vectors
\newcommand{\ones}{\m1}             % vector of all ones
\newcommand{\zeros}{\m0}            % vector of all zeros
\newcommand{\diag}{\mbox{diag}}
\newcommand{\Diag}{\mbox{Diag}}
\newcommand{\dilation}{\mbox{dilation}}
\newcommand{\congestion}{\mbox{congestion}}
\newcommand{\stretchh}{\mbox{stretch}}
\newcommand{\Reff}{R^{\mbox{\scriptsize eff}}}  % effective resistance
\newcommand{\pinv}{^{\dagger}}                  % pseudoinverse
\newcommand{\Real}{\mathbb{R}}      % real numbers
\newcommand{\krylov}{\mathcal{K}}   % Krylov subspace

\topmargin 0in
\textheight 7.9in
\oddsidemargin 0pt
\evensidemargin 0pt
\textwidth 6.5in

\begin{document}

\title{Index of Notation and Definitions}
\author{CS 292F: Graph Laplacians and Spectra}
\date{Version of April 22, 2021}
\maketitle

There is a lot of variation in terminology and notation in
the field of Laplacian matrix computation and spectral graph 
theory.  
Indeed, even ``Laplacian matrix'' is defined differently by
different authors!

This list gives the versions of notation, terminology, and definitions 
that we will use in CS 292F.
I mostly follow the conventions of Dan Spielman's notes, 
though I prefer not to use greek letters for vectors.
I will keep adding to this list during the quarter.

\begin{enumerate}

\item
Unless otherwise stated, a {\em graph} $G = (V,E)$ is always 
an undirected graph whose $n$ vertices are the integers 
$1$ through $n$, with no multiple edges or loops.

\item
The {\em degree} of a vertex is the number of edges incident on it, 
or equivalently (because we don't allow multiple edges or loops)
the number of its neighboring vertices.

\item 
A graph is said to be {\em regular} if every vertex has the same degree.

\item
A graph is said to be {\em connected} if, for every choice of two
vertices $a$ and~$b$, there is a {\em path} of edges from $a$ to~$b$.
The {\em connected components} of a graph are its maximal connected
subgraphs.

\item
$K_n$ is the {\em complete graph}, which has $n$ vertices and all $n(n-1)/2$ possible edges.

\item
$P_n$ is the {\em path graph}, which has $n$ vertices and $n-1$ edges in a single path.

\item
$S_n$ is the {\em star graph}, which has $n$ vertices, one with degree $n-1$ and 
$n-1$ with degree 1.

\item
$H_k$ is the {\em hypercube graph}, which has $n=2^k$ vertices, all of degree $k$.
Vertices $i$ and $j$ have an edge between them if $i$ and $j$ differ by a power of 2.
Equivalently, we can identify each vertex with a subset of $\{1,\ldots,k\}$,
with edges to just those subsets formed by adding or deleting one element.

\item
$G_e$ or $G_{(a,b)}$ is the graph with $n$ vertices and only one edge $e = (a,b)$.

\item
We will write a {\em vector} as a lower-case latin letter, 
possibly with a subscript, like $x$ or $w_2$.  
We often think of an $n$-vector as a set of labels for the
$n$ vertices of a graph; 
in that case element $i$ of vector $x$ is written as $x(i)$,
and we may write $x\in\Real^V$ instead of $x\in\Real^n$.
In linear algebraic expressions, vectors are column vectors.

\item
Two special vectors are $\zeros$, the vector of all zeros,
and $\ones$, the vector of all ones.

\item
If $a$ is a vertex then $\ones_a$~is the 
{\em characteristic vector} of~$a$, which
is zero except for $\ones_a(a)=1$.
Similarly if $S$~is a set of vertices, 
then $\ones_S$ is the vector that is equal to one
on the elements of~$S$ and zero elsewhere.

\item
If $x$ and $y$ are vectors of the same dimension, 
$$x^Ty = y^Tx = \sum_{i=0}^{n}x(i)y(i)$$
is their inner product (or dot product).
Thus $\ones^T\!x$ is the sum of the elements of $x$,
and $x^Tx$ is the square of the 2-norm (Euclidean length) of $x$.
If $x^Ty=0$, we call $x$ and $y$ {\em orthogonal}, 
and they are in fact perpendicular as vectors in $\Real^n$.

\item
If $d$ is an $n$-vector, $\Diag(d)$ is the $n$-by-$n$ diagonal 
matrix with the elements of $d$ on the diagonal.
If $A$ is any $n$-by-$n$ matrix, $\diag(A)$ is the $n$-vector
of the diagonal elements of $A$.

\item\label{lap}
The {\em Laplacian} of graph $G$ is the $n$-by-$n$ matrix $L$
whose diagonal element $L(a,a)$ is the degree of vertex $a$, 
and whose off-diagonal element $L(a,b)$ is~$-1$ if $(a,b) \in E$ 
and $0$ if $(a,b) \notin E$.
This matrix, which we (and Spielman) just call the Laplacian,
is sometimes called the {\em combinatorial Laplacian} to 
distinguish it from the normalized Laplacian below~(\ref{nlap}).
Note that $ L\ones = \zeros$.

\item
$L_e$ or $L_{(a,b)}$ is the $n$-by-$n$ Laplacian matrix
of the graph with $n$ vertices and only one edge $e = (a,b)$.
This matrix has only four nonzero elements, two 1's on the
diagonal and two $-1$'s in positions $(a,b)$ and $(b,a)$;
thus 
$$L_{(a,b)}=(\ones_a-\ones_b)(\ones_a-\ones_b)^T.$$
The Laplacian of any graph $G=(V,E)$ is the sum of the Laplacians
of its edges,
$$L_G = \sum_{e\in E} L_e.$$

\item\label{LQF}
The {\em Laplacian quadratic form} (or just LQF) is $x^TLx$,
where $L$ is a particular graph's Laplacian and $x$ is a variable $n$-vector.
Its value for a particular vector $x$ is 
$$x^TLx = \sum_{(a,b)\in E}(x(a)-x(b))^2.$$

\item
A {\em cut vector} is a vector each of whose elements is $+1$ or $-1$.
We can think of a cut vector~$x$ as representing a {\em cut} that partitions
the vertices of graph into two sets $S = \{\,a : x(a) = 1\,\}$
and $V-S = \{\,a : x(a) = -1\,\}$.
The LQF evaluated at a cut vector is easily seen to be four times the number 
of edges that cross the cut:
$$ x^TLx = 4\cdot|\{\,(a,b)\in E : a\in S \wedge b\in V-S\,\}|.$$

\item\label{eig}
If $Aw=\lambda w$ for any square matrix~$A$, nonzero vector~$w$, and scalar~$\lambda$, 
then $\lambda$ is an {\em eigenvalue} of~$A$ and $w$~is an {\em eigenvector}
associated with~$\lambda$.

\item
If $A$ is square and $B$~is nonsingular, then the eigenvalues of $BAB^{-1}$ are the
same as those of~$A$, 
and the eigenvectors of $BAB^{-1}$ are $B$ times the eigenvectors of~$A$.

\item
Every Laplacian $L$ is {\em positive semidefinite}, 
which (along with symmetry) implies that its $n$ eigenvalues are nonnegative and real.
Zero is an eigenvalue of $L$ with multiplicity
equal to the number of connected components of the graph $G$.  
Therefore,
if $G$ is connected, we have $0 = \lambda_1 < \lambda_2 \leq \cdots \leq \lambda_n$.
In that case the eigenvector $w_1$ is the constant vector $\ones/\sqrt n$.

\item
The {\em Fiedler value} of a graph is $\lambda_2$, its second-smallest eigenvalue,
and the {\em Fiedler vector} is $w_2$, the associated eigenvector.
The Fiedler value of a graph is also called its {\em algebraic connectivity}.
Note that $\lambda_2=0$ iff the graph is not connected.

\item
A square matrix $Q$ is {\em orthogonal} if $Q^TQ=I$, that is, its inverse
is its transpose.  
As vectors, the columns of $Q$ have unit length and are pairwise perpendicular; 
the same is true of the rows of $Q$.

\item\label{symeig}
If the $n$-by-$n$ matrix $A$ is symmetric, then it possesses $n$ real eigenvalues
$\lambda_1\leq\lambda_2\leq\cdots\leq\lambda_n$ (possibly including duplicates) 
associated with $n$ mutually orthogonal unit-length eigenvectors
$w_1, w_2, \ldots, w_n$.  If $W$ is the matrix $[w_1\; w_2\; \ldots\; w_n]$
and $\Lambda$ is the matrix $\Diag(\lambda_1,\ldots,\lambda_n)$
then we can summarize this as $AW = W\Lambda$ and $W^TW=I$.
We also have $A=W\Lambda W^T$, whence
$$A = \sum_{i=1}^n \lambda_i w_iw_i^T.$$

\item
If symmetric $A$ and its eigenvalues and eigenvectors are as in~(\ref{symeig}), 
any vector $x$ can be written as a linear combination of eigenvectors,
$$x = \sum_{i=1}^n \alpha_i w_i,$$
where $\alpha_i = w_i^Tx.$  
Multiplication by $A$ acts termwise on such a sum:
$$A^k x = \sum_{i=1}^n \alpha_i\lambda_i^k w_i.$$

\item
If symmetric $A$ and its eigenvalues and eigenvectors are as in~(\ref{symeig}), 
the {\em pseudoinverse} of $A$ is
$$A\pinv = \sum_{\lambda_i\ne 0} \frac{1}{\lambda_i} w_iw_i^T,$$
where the sum is taken over the nonzero eigenvalues of $A$.
If $A$ is nonsingular, $A\pinv=A^{-1}$.
If $x$ is orthogonal to the null space of $A$ 
(i.e.\ $x^Tw_i=0$ whenever $\lambda_i=0$), then
$$A\pinv A x = AA\pinv x = x.$$

\item
The {\em positive semidefinite square root} of a positive semidefinite matrix~$A$
with eigenvalues and eigenvectors as in~(\ref{symeig}) is the matrix
$$A^{1/2} = \sum_{i=1}^n \lambda_i^{1/2} w_iw_i^T.$$
We write the psd square root of~$A\pinv$ as 
$$A^{\dagger/2} = \sum_{\lambda_i\ne 0} \lambda_i^{-1/2} w_iw_i^T.$$

\item
The {\em Rayleigh quotient} of a nonzero vector $x$ and a matrix $A$ is
$$\frac{x^TAx}{x^Tx}.$$
If $Ax=\lambda x$, then the Rayleigh quotient of $x$ and $A$ is $\lambda$.

\item\label{RQT}{\bf Rayleigh quotient theorem}.
The eigenvectors of a symmetric matrix $A$ are critical points of its
Rayleigh quotient (considered as a real-valued function of an $n$-vector).
Specifically, 
$$\lambda_k = \min_{x\perp w_1,\ldots,w_{k-1}} \frac{x^TAx}{x^Tx}
            = \max_{x\perp w_{k+1},\ldots,w_n} \frac{x^TAx}{x^Tx},$$
and the extreme values are attained at $x = w_k$.
In particular, therefore, for a Laplacian $L$ the Fiedler value is
$$\lambda_2 = \min_{\ones^T\!x=0} \frac{x^TAx}{x^Tx},$$
attained at the Fiedler vector $w_2$.

\item\label{CFT}{\bf Courant-Fischer theorem}
(another version of the Rayleigh quotient theorem).
The eigenvalues $\lambda_1\leq\cdots\leq\lambda_n$ of a symmetric matrix $A$ 
are characterized by
$$\lambda_k = \max_{\dim \mathbb{S}=n-k+1}\;\min_{x\in \mathbb{S}}\frac{x^TAx}{x^Tx}
            = \min_{\dim \mathbb{S}=k}\;\max_{x\in \mathbb{S}}\frac{x^TAx}{x^Tx},$$
where $\mathbb{S}$ ranges over subspaces of $\Real^n$.
The extreme values are attained at $x = w_k$.

\item
A {\em test vector for $\lambda_2$} is an $n$-vector that is
orthogonal to $\ones$.  By the Raleigh quotient theorem,
if $v$ is any test vector then $\lambda_2 \leq v^TLv/v^Tv$.
Note that any vector $x$ can be converted to a test vector 
$v = x - (\ones^T\!x/n) \ones$; in words, subtracting off
the mean of any vector orthogonalizes it against the constant
vector.

\item
The {\em boundary} of a set $S\subseteq V$ of vertices, written $\partial S$,
is the set of edges with just one endpoint in $S$. Formally,
$\partial S = \{\,(a,b)\in E : a \in S \wedge b \in V-S\,\}$.
The number of edges in $\partial S$ is $|\partial S|$.

\item\label{isop}
The {\em isoperimetric ratio} of a set $S\subseteq V$ of vertices,
written $\theta(S)$, is the ratio
$$\theta(S) = \frac{|\partial S|}{|S|}.$$
This is one sort of ``surface-to-volume ratio''; 
see the definition of conductance~(\ref{cond}) 
for another.

\item\label{isopnum}
The {\em isoperimetric ratio} of a graph $G$,
written $\theta_G$, is the smallest isoperimetric ratio over all 
sets with at most half the vertices,
$$\theta_G = \min_{|S|\leq n/2}\theta(S).$$
Note that $\theta_G=0$ if and only if $G$ is not connected.

\item\label{isopthm} {\bf Isoperimetric theorem}.
For any set $S$ of vertices, 
$$\theta(S) \geq \lambda_2(1-|S|/n).$$
It follows that the isoperimetric ratio of the graph is bounded 
in terms of the Fiedler value,
$$\theta_G \geq \lambda_2/2.$$
This says that the larger $\lambda_2$ is, the larger the surface-to-volume
ratio of any relatively small set of vertices must be.

\item\label{cond}
The {\em conductance} of a set $S\subseteq V$ of vertices,
written $\phi(S)$, is the ratio 
$$\phi(S) = \frac{|\partial S|}{\min(d(S),d(V-S))},$$
where $d(S)$ is the sum of the degrees of the vertices in $S$.
This is another sort of ``surface-to-volume ratio'';
isoperimetric number~(\ref{isopnum}) measures volume just by counting vertices, 
while conductance measures volume by counting vertices weighted by their degrees.
In class we defined conductance for unweighted graphs,
but the definition extends to weighted graphs~(\ref{wgraph}) with
a suitable interpretation of~$d(S)$.
(``Conductance'' has a different meaning in resistive networks, as we'll see later.)
%; see~(\ref{resistive}) below.)

\item
The {\em conductance} of a graph $G$,
written $\phi_G$, is the smallest conductance of any nonempty 
proper subset of vertices,
$$\phi_G = \min_{S\subset V}\phi(S).$$
This is sometimes called the ``Cheeger constant''
of the graph, but definitions are particularly
variable here and we'll stick to this one.
Note that $\phi_G=0$ iff $G$ is not connected.
(``Conductance'' has a different meaning in resistive networks, as we'll see later.)
%; see~(\ref{resistive}) below.)

\item\label{nlap}
The {\em normalized Laplacian} of graph~$G$ is 
the $n$-by-$n$ matrix~$N$
whose diagonal element $N(a,a)$ is equal to 1,
and whose off-diagonal element $N(a,b)$ 
is~$-1/\sqrt{d(a)d(b)}$, 
where we define $d$ to be the vector of vertex degrees of~$G$.
Another way to say it is that the normalized Laplacian is 
the (ordinary) Laplacian with rows and columns scaled symmetrically 
to make the diagonal elements equal to~1. 
If $D=\diag(d)$ is the diagonal matrix of degrees, 
then 
$$N = D^{-1/2}LD^{-1/2}.$$
Some authors, including notably Fan Chung in her 
wonderful book {\em Spectral Graph Theory}, 
use the name ``Laplacian'' for this matrix~$N$
instead of for our~$L$.

\item\label{nlapeig}
The normalized Laplacian~$N$ is symmetric and 
positive semidefinite, and like the Laplacian
it has 0 as an eigenvalue with multiplicity
equal to the number of connected components
of~$G$.  In general however $N$'s eigenvalues
and eigenvectors are different from $L$'s.
We write $0=\nu_1\le\nu_2\le\cdots\le\nu_n$
for the eigenvalues of~$N$.
The eigenvector corresponding to~$\nu_1$ is
not the constant vector, but the vector~$d^{1/2}$
of the square roots of the vertex degrees:
$$Nd^{1/2} = D^{-1/2}LD^{-1/2}d^{1/2} = D^{-1/2}L\ones = D^{-1/2}\zeros = \zeros.$$

\item\label{nlaprq}
The Rayleigh quotient for the normalized Laplacian~$N$, 
whose critical points determine the eigenvalues,
is related to a ``generalized Rayleigh quotient'' for the Laplacian~$L$.
Specifically, we have
$$\frac{x^TNx}{x^Tx} = \frac{y^TLy}{y^TDy},$$
where $D=\Diag(d)$ is the diagonal matrix of vertex degrees 
and $y=D^{-1/2}x$.
Thus the eigenvalues of $Nx=\nu x$ come from
the generalized eigenvalue problem $Ly=\nu Dy.$

\item\label{gershgorin}{\bf Gershgorin's theorem}.
If $A$ is any square matrix (real or complex), its $n$~eigenvalues are all contained
in the union of the $n$~disks $D_1,\ldots,D_n$ in the complex plane defined by
$$D_a = \{\alpha : |\alpha-A(a,a)| \leq \sum_{b\neq a}|A(a,b)|\}.$$
This implies, for example, that the largest eigenvalue~$\lambda_n$ of a Laplacian
is at most twice the maximum vertex degree.

\item
It follows from Gershgorin's theorem~(\ref{gershgorin}) that
the eigenvalues of the normalized Laplacian $N$ are always bounded by~0 and~2,
$$0 = \nu_1 \leq \nu_2 \leq \cdots \leq \nu_n \leq 2.$$

\item\label{cheeger} {\bf Cheeger's inequalities}.
The normalized Laplacian can be used to give both 
upper and lower bounds on the conductance,
$$\nu_2/2 \leq \phi_G \leq \sqrt{2\nu_2}.$$
Equivalently,
$$\phi_G^2/2 \leq \nu_2 \leq 2\phi_G.$$
The upper bound on~$\nu_2$ is analogous to the
isoperimetric inequality~(\ref{isopthm}).
The lower bound on~$\nu_2$ is Cheeger's inequality,
one of the most significant theorems of spectral
graph theory.
In class we stated (and partly proved) these
inequalities for unweighted graphs, but they 
hold for weighted graphs~(\ref{wgraph}) as well; 
the Spielman book proves the weighted version.

\item\label{csi} {\bf Cauchy-Schwarz inequality}.
Just for reference, because it comes up in several of
the proofs we're looking at.  
If $x$ and~$y$ are $n$-vectors, then 
$$|x^Ty| \leq \|x\|\,\|y\|.$$
Equivalently,
$$\Big(\sum_i x(i)y(i)\Big)^2 \leq 
\Big(\sum_i x(i)^2\Big)
\Big(\sum_i y(i)^2\Big).$$

\item\label{wgraph}
A {\em weighted graph} is an undirected graph that comes 
with {\em positive} weights on the edges, which we write~$c(e)$ or~$c(a,b)$.
We take $c(a,b)=0$ if $(a,b)$ is not an edge; 
weights on edges are required to be strictly positive.
Note that $c(a,b)=c(b,a)$.
We can think of all graphs as weighted graphs;
an ``unweighted'' graph just has edge weights all equal to~1.

\item
In a weighted graph, we often interpret~$d(a)$, for a vertex~$a$,
not as the number of incident edges but as the sum of the weights
of the incident edges:
$$d(a) = \sum_{b \neq a}c(a,b),$$
and we often define the diagonal matrix $D = \Diag(d)$ as the matrix
whose entries are those sums.
We also (as before) write~$d(S)$, where $S$~is a set of vertices, 
to mean $\sum_{a \in S}d(a)$.

\item\label{wlap}
The {\em Laplacian matrix of a weighted graph}
is the $n$-by-$n$ matrix~$L$ whose off-diagonal
element~$L(a,b)$ is~$-c(a,b)$ if $(a,b) \in E$ and $0$ if $(a,b) \notin E$,
and whose diagonal element $L(a,a) = d(a) = \sum_{b \neq a}c(a,b)$ is chosen to
make the row sums zero.
Like the Laplacian of an unweighted graph, we have $L\ones=\zeros$, 
and indeed 0~is an eigenvalue of~$L$ with multiplicity equal to the 
number of connected components of the graph.
For an unweighted graph, this is equivalent to our previous definition,
with all edge weights equal to~1.

\item\label{wnorm}
The {\em normalized Laplacian matrix of a weighted graph}
is the matrix~$N$
whose diagonal element~$N(a,a)$ is equal to~1,
and which for each edge~$(a,b)$ has symmetric off-diagonal elements 
$N(a,b) = N(b,a) = -c(a,b)/\sqrt{d(a)d(b)}$.
Here $d(a)$ is the sum of the weights of edges incident on~$a$.
If $D = \Diag(d)$ is the diagonal matrix of those sums
and $L$ is the Laplacian of the weighted graph, then
$$N = D^{-1/2}LD^{-1/2}.$$
Like the normalized Laplacian of an unweighted graph, 
we have $Nd^{1/2}=\zeros$, 
and 0 remains an eigenvalue of~$N$ with multiplicity equal to the 
number of connected components of the graph.
Again this is equivalent to our previous definition for an unweighted graph
if all edge weights are equal to~1.

\item {\bf Multiple of a graph}.
If $G$ is a (weighted) graph and $\alpha>0$ is a constant, $\alpha G$~is the
graph whose edge weights are all multiplied by~$\alpha$.
The ordinary Laplacian of~$\alpha G$ is $\alpha$~times the Laplacian of~$G$,
$$L_{\alpha G} = \alpha L_G.$$ 
On the other hand, the normalized Laplacian of $\alpha G$ is the same as the
normalized Laplacian of~$G$, 
$$N_{\alpha G} = N_G,$$ 
since the normalization wipes out the factor of~$\alpha$.

\item {\bf Semidefinite ordering}.
If $A$ is a matrix, $A\succeq0$ means that $A$ is symmetric and positive semidefinite.
Thus $L\succeq 0$ for any Laplacian~$L$.
If $A$ and~$B$ are matrices, $A\succeq B$ means $A-B\succeq 0$.
If $G$ and~$H$ are graphs or weighted graphs, $G\succeq H$ means $L_G\succeq L_H$
(note that we are using the ordinary, un-normalized Laplacian here).
Then $G\succeq H$ if and only if $x^TL_Gx \geq x^TL_Hx$ for all vectors~$x$.
For matrices $A \succeq 0$ and $B \succeq 0$,
$A\succeq B$ implies $\lambda_k(A) \geq \lambda_k(B)$ for all~$k$,
but the converse is false.
Also, $A \succeq B$ implies $B\pinv \succeq A\pinv$.

\item {\bf Graph approximation}.
For any constant $\alpha\geq 1$, (weighted) graph~$H$ is an {\em $\alpha$-approximation} 
of (weighted)~graph $G$ if $\alpha H \succeq G \succeq H/\alpha$.
This definition actually applies to all symmetric matrices, not just graph Laplacians.

\end{enumerate}

\end{document}
